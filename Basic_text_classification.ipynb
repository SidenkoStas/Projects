{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, losses, preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "# Скачиваем файл по ссылке.\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                 untar=True, cache_dir='.',\n",
    "                                 cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb') # Создаем ссылку на директорию датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdb.vocab', 'imdbEr.txt', 'README', 'test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir) # Смотрим содержимое в директории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, 'train') # Создаем ссылку на дирректорию тренировочнойпапки.\n",
    "os.listdir(train_dir) # Смотрим содержимое в директории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
     ]
    }
   ],
   "source": [
    "# Смотрим один из примеров позитивного ревью.\n",
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, 'unsup') # Создаем ссылку на дирректорию для удаления.\n",
    "shutil.rmtree(remove_dir) # Удаляем всю ветку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "seed = 113\n",
    "# Из дерриктории создаем датасеты train&valid.\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(train_dir, batch_size=batch_size,\n",
    "                                                             validation_split=0.2,\n",
    "                                                             subset='training',\n",
    "                                                             seed=seed)\n",
    "raw_valid_ds = tf.keras.preprocessing.text_dataset_from_directory(train_dir, batch_size=batch_size,\n",
    "                                                             validation_split=0.2,\n",
    "                                                             subset='validation',\n",
    "                                                             seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b'This is a lovely tale of guilt-driven obsession.<br /><br />Matiss, on a lonely night stroll in Riga (?) passes by a woman on the wrong side of a bridge railing. He passes by without a word. Only the splash in the water followed by a cry for help causes him to act. And then only too little and too late.<br /><br />The film chronicles his efforts at finding out more about the woman. On a troll of local bars, he finds her pocketbook. He pieces more and more of her life together. His \"look\" changes as his obsession grows. He has to make things right. In a marvelously filmed dialog with the \"bastard ex-boyfriend\" he forces Alexej to face up to the guilt that both feel.<br /><br />Haunting long takes, a gritty soundtrack to accentuate the guilt, barking dogs. Footsteps. Lovely film noir with a lovely twist. A good Indie ending.'\n",
      "Label 1\n",
      "Review b'This effort is based on the true story of Jim Morris, a high school science teacher/baseball coach, who is inspired by his players to try out for the pros and fulfill his life-long dream of playing in the majors. Dennis Quaid, no stranger to sports films, plays Morris with enough conviction to make the part work and the producers do a credible job of recreating the real-world events that led to Morris brief stint as a relief pitcher for the woefull Tampa Bay Devil Rays. The first half of the film, dealing with his rag tag bunch of High School Baseball players (all of whom look way too old to actualy be in High School) is less effective and probably a bit too long. Overall the film does suffer from some pacing issues and a few extra subplots that we probably could have done without. However, it is still a fairly involving movie with an inspirational theme that proves once again that baseball is the national pastime for a reason. GRADE: B-'\n",
      "Label 1\n",
      "Review b\"Marjorie, a young woman who works in a museum and lives with two female roommates, Pat and Terry.One night she gets in her car and is attacked by masked man with a knife.His plan is to rape her, but she manages to escape.The man has her purse.The police can't help her, since the actual rape didn't happen.Then one day, when Marjorie's roommates are at work, her assailant comes there.His name is Joe.A long battle begins against this man.But then she manages to spray his eyes and mouth with insect repellent, stuff that will kill him if he won't get help soon.She ties him up and makes Joe the subject of the same kind of physical and mental assaults he used on her earlier.The Extremities (1986) is directed by Robert M. Young.It's based on the controversial off-Broadway play from 1982 by William Mastrosimone.Farrah Fawcett, who sadly lost her battle with cancer last year, is terrific as Marjorie.James Russo, who played the attacker also in the play, is convincing as Joe.Alfre Woodard and Diana Scarwid are great as Pat and Terry.James Avery is seen as Security Guard.She got a Golden Globe nomination.This is not a movie that is supposed to entertain you.It asks a question is revenge justified.This is not a perfect movie, but I recommend it.\"\n",
      "Label 1\n"
     ]
    }
   ],
   "source": [
    "# Просмотрим пару вариантов.\n",
    "for text_batch, label_batch in raw_train_ds.take(1): # Берем один элемент.\n",
    "    for i in range(3):\n",
    "        print(\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to neg\n",
      "Label 1 corresponds to pos\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0]) # Названия классов.\n",
    "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Тестовый датасет из директории.\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартизация, собственная функция для удаления меток HTML, пунктуации и понижение регистра.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html, f'{re.escape(string.punctuation)}', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Слой векторизации\n",
    "max_features = 1000\n",
    "sequence_length = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(standardize=custom_standardization,\n",
    "                                   max_tokens=max_features,\n",
    "                                   output_mode='int',\n",
    "                                   output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем только текст и адаптируем векторизацию для создания словаря.\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review  tf.Tensor(b'Prominent attorney Walter Pidgeon takes a murder case pro bono, wins an acquittal and discovers that his client (Keefe Braselle) was not only guilty but part of an extortion ring reaching to the highest eschelons of the city. Panged by his own complicity, he undertakes an investigation, stumbles onto the identity of the \"unknown man\" who heads the syndicate, and murders him.<br /><br />The ironies engage when Braselle is charged with this second murder and Pidgeon must defend him by pointing to the existence of another \"unknown man\" -- himself. Though somewhat short of urban grit and long on rhetoric, the Unknown Man belongs to the noir cycle less by style or structure than by its acknowledgement of the pervasive corruption of American municipal politics that came to light in the postwar years.', shape=(), dtype=string) \n",
      "\n",
      "Label  pos \n",
      "\n",
      "Vectorized review (<tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
      "array([[  1,   1,   1,   1, 268,   3, 741, 525,   1,   1,   1,  31,   1,\n",
      "          4,   1,  11,  20,   1,   1,   1,  13,  21,  57,   1,  17, 181,\n",
      "          5,  31,   1,   1,   1,   6,   2,   1,   1,   5,   2,   1,   1,\n",
      "         30,  20, 207,   1,  26,   1,  31,   1,   1,   1,   2,   1,   5,\n",
      "          2,   1,   1,  35,   1,   2,   1,   4,   1, 403,   2,   1,   1,\n",
      "         49,   1,   7,   1,  16,  10, 327, 741,   4,   1, 205,   1,  97,\n",
      "         30,   1,   6,   2,   1,   5, 158,   1,   1, 317,   1, 185, 587,\n",
      "        401,   5,   1,   1,   4, 226,  18,   1,   2]], dtype=int64)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим как работает функция векторизации.\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print('Review ', first_review, '\\n')\n",
    "print('Label ', raw_train_ds.class_names[first_label], '\\n')\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 --->  movie,\n",
      "478 --->  white\n",
      "Vocabulary size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим какому индексу соответствуют слова.\n",
    "print(\"149 ---> \",vectorize_layer.get_vocabulary()[149])\n",
    "print(\"478 ---> \",vectorize_layer.get_vocabulary()[478])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим векторизацию ко всем данным.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "valid_ds = raw_valid_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "valid_ds = valid_ds.cache().prefetch(AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем момдель.\n",
    "embedding_dim = 32\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, embedding_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = layers.Embedding(max_features+1, embedding_dim)\n",
    "        self.dropout_1 = layers.Dropout(0.2)\n",
    "        self.pooling = layers.GlobalAveragePooling1D()\n",
    "        self.dropout_2 = layers.Dropout(0.2)\n",
    "        self.dence = layers.Dense(1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout_2(x)\n",
    "        return self.dence(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель.\n",
    "model = MyModel(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры.\n",
    "optimazer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем метрики потери и точноти для всех датасетов отдельно.\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.0)\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "valid_accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.0)\n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(threshold=0.0)\n",
    "test_loss = tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для тренировки модели.\n",
    "def train(texts, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(texts, training=True)\n",
    "        loss = loss_fn(labels, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimazer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для валидации модели.\n",
    "def valid(texts, labels):\n",
    "    prediction =  model(texts, training=False)\n",
    "    v_loss = loss_fn(labels, prediction)\n",
    "    \n",
    "    valid_loss(v_loss)\n",
    "    valid_accuracy(labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для валидации модели.\n",
    "def test(texts, labels):\n",
    "    prediction = model(texts, training=False)\n",
    "    t_loss = loss_fn(labels, prediction)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Train loss: 0.42365968227386475\n",
      "Train accuracy: 0.8053996562957764\n",
      "Valid loss: 0.4708956480026245\n",
      "Valid accuracy: 0.767799973487854\n",
      "Epoch: 2\n",
      "Train loss: 0.4231114983558655\n",
      "Train accuracy: 0.8051995635032654\n",
      "Valid loss: 0.4716337323188782\n",
      "Valid accuracy: 0.767599880695343\n",
      "Epoch: 3\n",
      "Train loss: 0.42381227016448975\n",
      "Train accuracy: 0.8045499920845032\n",
      "Valid loss: 0.471934050321579\n",
      "Valid accuracy: 0.7671999335289001\n",
      "Epoch: 4\n",
      "Train loss: 0.42394396662712097\n",
      "Train accuracy: 0.8046997785568237\n",
      "Valid loss: 0.4723937213420868\n",
      "Valid accuracy: 0.7669999003410339\n",
      "Epoch: 5\n",
      "Train loss: 0.4222526550292969\n",
      "Train accuracy: 0.8049994111061096\n",
      "Valid loss: 0.47280704975128174\n",
      "Valid accuracy: 0.7671999335289001\n",
      "Epoch: 6\n",
      "Train loss: 0.4223613739013672\n",
      "Train accuracy: 0.8061996698379517\n",
      "Valid loss: 0.47312507033348083\n",
      "Valid accuracy: 0.7683998942375183\n",
      "Epoch: 7\n",
      "Train loss: 0.42297253012657166\n",
      "Train accuracy: 0.8039497137069702\n",
      "Valid loss: 0.473359614610672\n",
      "Valid accuracy: 0.7685997486114502\n",
      "Epoch: 8\n",
      "Train loss: 0.4230065941810608\n",
      "Train accuracy: 0.8041999340057373\n",
      "Valid loss: 0.4734247326850891\n",
      "Valid accuracy: 0.7677997350692749\n",
      "Epoch: 9\n",
      "Train loss: 0.4225398302078247\n",
      "Train accuracy: 0.807049572467804\n",
      "Valid loss: 0.47375166416168213\n",
      "Valid accuracy: 0.7677997350692749\n",
      "Epoch: 10\n",
      "Train loss: 0.42258167266845703\n",
      "Train accuracy: 0.8049999475479126\n",
      "Valid loss: 0.4741361141204834\n",
      "Valid accuracy: 0.7685997486114502\n"
     ]
    }
   ],
   "source": [
    "# Запускаем тренировку модели\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Стираем все данные до начал следующей эпохи\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    \n",
    "    for text, labels in train_ds:\n",
    "        train(text, labels)\n",
    "        \n",
    "    for texts, labels in valid_ds:\n",
    "        valid(texts, labels)\n",
    "        \n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "    print(f'Train loss: {train_loss.result()}')\n",
    "    print(f'Train accuracy: {train_accuracy.result()}')\n",
    "    print(f'Valid loss: {valid_loss.result()}')\n",
    "    print(f'Valid accuracy: {valid_accuracy.result()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss.reset_states()\n",
    "test_accuracy.reset_states()\n",
    "\n",
    "for texts, labels in test_ds:\n",
    "    test(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Test loss: {test_loss.result()}')\n",
    "print(f'Test accuracy: {test_accuracy.result()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
